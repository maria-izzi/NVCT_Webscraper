{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maria Izzi\n",
    "#July 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary packages\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import PySimpleGUI as sg\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up GUI\n",
    "sg.theme('Black')\n",
    "layout = [[sg.Text('Instructions: tinyurl.com/nvctscraper')],\n",
    "          [sg.Text('Input file:'), sg.InputText()],\n",
    "          [sg.Text('Parcel ID column name:'), sg.InputText()],\n",
    "          [sg.Text('County:')],\n",
    "          [sg.Listbox(['Arlington','Fairfax','Loudoun','Stafford'],size=(20,4))],\n",
    "          [sg.Text('County parcel ID search website link:'), sg.InputText()],\n",
    "          [sg.Button('Done')]]\n",
    "\n",
    "window = sg.Window('NVCT Parcel Web Scraper', layout)\n",
    "\n",
    "while True:\n",
    "    event, values = window.read()\n",
    "    if event == sg.WIN_CLOSED or event == 'Cancel' or event=='Done': # if user closes window or clicks cancel\n",
    "        break\n",
    "    print('You entered ', values)\n",
    "\n",
    "window.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking inputs from GUI\n",
    "inp_file = values[0]\n",
    "col_name = values[1]\n",
    "county = values[2]\n",
    "link = values[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data\n",
    "data = pd.read_csv(inp_file,dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ARLINGTON (Stafford is similar)\n",
    "#setting up list for invalid parcel IDs\n",
    "invalids = []\n",
    "if(county[0]=='Arlington'):\n",
    "    #setting up webdriver\n",
    "    driver = webdriver.Edge('./msedgedriver')\n",
    "    \n",
    "    #setting up output dataframe\n",
    "    df = pd.DataFrame(data={'PIN':[],'Name':[],'Address':[]})\n",
    "    \n",
    "    #count for adding next line to output dataframe\n",
    "    count = 0\n",
    "    \n",
    "    #iterating through each parcel ID in the given column in the data\n",
    "    for pin in data[col_name]:\n",
    "        try:\n",
    "            #open the website\n",
    "            driver.get(link)\n",
    "            \n",
    "            #search for the search box and enter the parcel ID\n",
    "            temp = driver.find_element_by_id(\"SearchFilters_RPCs\")\n",
    "            temp.send_keys(pin)\n",
    "            temp.submit()\n",
    "            \n",
    "            #open a BeautifulSoup parser for the website\n",
    "            src = driver.page_source\n",
    "            soup = bs(src, 'html.parser')\n",
    "            \n",
    "            #open the link for the page with the parcel owner(s) name and address\n",
    "            allread = soup.find_all('a',string='View')\n",
    "\n",
    "            viewlink = \"\"\n",
    "            for i in allread:\n",
    "                viewlink = i.get('href')\n",
    "\n",
    "            r = requests.get(\"https://propertysearch.arlingtonva.us/\"+viewlink)\n",
    "            soup_arlington = bs(r.content,'html.parser')\n",
    "            arl_read = [soup_arlington.find_all(class_=\"one-half first\")[0].text,soup_arlington.find_all(class_=\"one-half first\")[1].text]\n",
    "\n",
    "            df.loc[count] = ([pin,arl_read[0].replace(\"Owner\",\"\",1).strip(),arl_read[1].replace(\"Mailing Address\",\"\",1).strip()])\n",
    "            count += 1\n",
    "            \n",
    "        #to make sure any invalid parcel IDs don't halt the whole process\n",
    "        except:\n",
    "            print(pin+\" is invalid.\")\n",
    "            invalids.append(pin)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAIRFAX (Loudoun is similar)\n",
    "invalids = []\n",
    "if(county[0]=='Fairfax'):\n",
    "    driver = webdriver.Edge('./msedgedriver')\n",
    "\n",
    "    df = pd.DataFrame(data={'PIN':[],'Name':[],'Address':[]})\n",
    "    count = 0\n",
    "\n",
    "    for pin in data[col_name]:\n",
    "        try:\n",
    "            driver.get(link)\n",
    "            temp = driver.find_element_by_id(\"inpParid\")\n",
    "            temp.send_keys(pin)\n",
    "            temp.send_keys(Keys.ENTER)\n",
    "            src = driver.page_source\n",
    "            soup = bs(src, 'html.parser')\n",
    "            \n",
    "            #no separate page to open, all data available on this page, so just selecting the data directly\n",
    "            allread = soup.find_all('td',\"DataletData\")\n",
    "            df.loc[count] = ([pin,allread[0].text,allread[1].text])\n",
    "            count += 1\n",
    "        except:\n",
    "            print(pin+\" is invalid.\")\n",
    "            invalids.append(pin)\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOUDOUN (Fairfax is similar)\n",
    "if(county[0]=='Loudoun'):\n",
    "    driver = webdriver.Edge('./msedgedriver')\n",
    "\n",
    "    df = pd.DataFrame(data={'PIN':[],'Name':[],'Care Of':[],'Address':[]})\n",
    "    count = 0\n",
    "\n",
    "    for pin in data[col_name]:\n",
    "        try:\n",
    "            driver.get(link)\n",
    "            temp = driver.find_element_by_id(\"inpParid\")\n",
    "            temp.send_keys(pin)\n",
    "            temp.send_keys(Keys.ENTER)\n",
    "            src = driver.page_source\n",
    "            soup = bs(src, 'html.parser')\n",
    "            allread = soup.find_all('td',\"DataletData\")\n",
    "            df.loc[count] = ([pin,allread[0].text,allread[1].text,allread[2].text+\"\\n\"+allread[4].text])\n",
    "            count += 1    \n",
    "        except:\n",
    "            print(pin+\" is invalid.\")\n",
    "            invalids.append(pin)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STAFFORD (Arlington is similar)\n",
    "invalids = []\n",
    "if(county[0]=='Stafford'):\n",
    "    driver = webdriver.Edge('./msedgedriver')\n",
    "    df = pd.DataFrame(data={'PIN':[],'Name':[],'Address':[]})\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for pin in data[col_name]:\n",
    "        try:\n",
    "            driver.get(link)\n",
    "            temp = driver.find_element_by_id(\"fldSearchFor\")\n",
    "            temp.send_keys(pin)\n",
    "            temp.submit()\n",
    "            src = driver.page_source\n",
    "            soup = bs(src, 'html.parser')\n",
    "            allread = soup.find_all('a',title=\"Click here to view information\")\n",
    "\n",
    "            viewlink = \"\"\n",
    "            for i in allread:\n",
    "                viewlink = i.get('href')\n",
    "\n",
    "            r = requests.get(\"http://va-stafford-assessor.publicaccessnow.com\"+viewlink)\n",
    "            soup_stafford = bs(r.content,'html.parser')\n",
    "            staff_read = soup_stafford.find_all(class_=\"ui-widget ui-widget-content center\")[0].text\n",
    "\n",
    "            staff_read_list = staff_read.strip().split(\"\\n\")\n",
    "            staff_read_list[0] = staff_read_list[0].strip()\n",
    "            staff_read_list[1] = staff_read_list[1].strip()+\"\\n\"+staff_read_list[2].strip()\n",
    "            \n",
    "            df.loc[count] = ([pin,staff_read_list[0],staff_read_list[1]])\n",
    "            count += 1\n",
    "        except:\n",
    "            print(pin+\" is invalid.\")\n",
    "            invalids.append(pin)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'webscraper_output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.theme('Black')   # Add a touch of color\n",
    "# All the stuff inside your window.\n",
    "layout = [[sg.Text('Done! The output file should be in the same location as the input file.')],\n",
    "          [sg.Text('These parcel IDs (if any) are invalid:')],\n",
    "          [sg.Text(invalids)]]\n",
    "\n",
    "# Create the Window\n",
    "window = sg.Window('NVCT Parcel Web Scraper', layout)\n",
    "# Event Loop to process \"events\" and get the \"values\" of the inputs\n",
    "while True:\n",
    "    event, values = window.read()\n",
    "    if event == sg.WIN_CLOSED: # if user closes window or clicks cancel\n",
    "        break\n",
    "\n",
    "window.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
